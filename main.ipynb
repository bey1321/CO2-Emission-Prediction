{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e51ed833",
   "metadata": {},
   "source": "# CO2 Emissions Prediction using Linear Regression\n\n## Overview\nThis notebook implements linear regression to predict CO2 emissions from vehicle characteristics using two approaches:\n1. **Gradient Descent** - Implemented from scratch\n2. **Scikit-learn LinearRegression** - Using the standard machine learning library\n\n## Dataset\n- **Source**: CO2 Emissions_Canada.csv\n- **Features**: \n  - Engine Size (L)\n  - Cylinders\n  - Fuel Consumption City (L/100 km)\n  - Fuel Consumption Highway (L/100 km)\n  - Fuel Consumption Combined (L/100 km)\n- **Target**: CO2 Emissions (g/km)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c51d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5c94b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fb2ff4",
   "metadata": {},
   "source": "## Data Loading and Preparation\n\nThe dataset is loaded and the relevant features are extracted for model training."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc033463",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = './CO2 Emissions_Canada.csv'\n",
    "dataset = pd.read_csv(dataset_path)\n",
    "\n",
    "features = [\n",
    "    \"Engine Size(L)\",\n",
    "    \"Cylinders\",\n",
    "    \"Fuel Consumption City (L/100 km)\",\n",
    "    \"Fuel Consumption Hwy (L/100 km)\",\n",
    "    \"Fuel Consumption Comb (L/100 km)\"\n",
    "]\n",
    "target = \"CO2 Emissions(g/km)\"\n",
    "\n",
    "X = dataset[features].values  # shape (n_samples, 5)\n",
    "y = dataset[target].values    # shape (n_samples,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851ee08b",
   "metadata": {},
   "source": "## Exploratory Data Analysis\n\nVisualizing the relationship between each feature and the target variable (CO2 emissions) using scatter plots to understand the data distribution and correlations."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766e5621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots of each feature vs target\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, feature in enumerate(features):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    sns.scatterplot(data=dataset, x=feature, y=target)\n",
    "    plt.title(f'{feature} vs {target}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb23f40",
   "metadata": {},
   "source": "## Feature Scaling\n\nStandardizing the features using z-score normalization to ensure all features contribute equally to the model and to improve gradient descent convergence."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4df10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean = X.mean(axis=0)\n",
    "X_std = X.std(axis=0)\n",
    "X_scaled = (X - X_mean) / X_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834f5883",
   "metadata": {},
   "source": "## Train-Validation Split\n\nSplitting the dataset into training (80%) and validation (20%) sets with random shuffling to ensure unbiased model evaluation."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9766cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "indices = np.arange(X_scaled.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "X_scaled = X_scaled[indices]\n",
    "y = y[indices]\n",
    "\n",
    "split_idx = int(0.8 * X_scaled.shape[0])\n",
    "X_train, X_val = X_scaled[:split_idx], X_scaled[split_idx:]\n",
    "y_train, y_val = y[:split_idx], y[split_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7ea69d",
   "metadata": {},
   "source": "## Gradient Descent Implementation (From Scratch)\n\nImplementing the core components of linear regression using gradient descent optimization."
  },
  {
   "cell_type": "markdown",
   "id": "eac74090",
   "metadata": {},
   "source": "### Linear Model Function\n\nDefining the hypothesis function for linear regression: **f(x) = Xw + b**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73b43ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_wb(X, w, b):\n",
    "    \"\"\"Predict y using linear model: y = Xw + b\"\"\"\n",
    "    return X @ w + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bf1d24",
   "metadata": {},
   "source": "### Cost Function\n\nComputing the Mean Squared Error (MSE) as the cost function to measure model performance."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a94e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b):\n",
    "    \"\"\"Compute Mean Squared Error cost\"\"\"\n",
    "    m = X.shape[0]\n",
    "    y_pred = f_wb(X, w, b)\n",
    "    return (1/(2*m)) * np.sum((y_pred - y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f450471",
   "metadata": {},
   "source": "### Gradient Computation\n\nCalculating the partial derivatives of the cost function with respect to weights and bias."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6010f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(X, y, w, b):\n",
    "    \"\"\"Compute gradients of the cost function w.r.t weights and bias\"\"\"\n",
    "    m = X.shape[0]\n",
    "    y_pred = f_wb(X, w, b)\n",
    "    error = y_pred - y\n",
    "    dw = (1/m) * (X.T @ error)\n",
    "    db = (1/m) * np.sum(error)\n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfe5078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, alpha, iterations, record_interval):\n",
    "    \"\"\"\n",
    "    Performs Gradient Descent to train linear regression.\n",
    "\n",
    "    Args:\n",
    "        X (numpy.ndarray): Feature matrix of shape (n_samples, n_features)\n",
    "        y (numpy.ndarray): Target vector of shape (n_samples,)\n",
    "        alpha (float): Learning rate\n",
    "        iterations (int): Number of iterations\n",
    "        record_interval (int): Interval at which to record cost and weights\n",
    "\n",
    "    Returns:\n",
    "        w (numpy.ndarray): Final weights\n",
    "        b (float): Final bias\n",
    "        cost_history (list): Cost at recorded iterations\n",
    "        w_history (list): Weights at recorded iterations\n",
    "        b_history (list): Bias at recorded iterations\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize weights and bias\n",
    "    w = np.zeros(X.shape[1])\n",
    "    b = 0\n",
    "    \n",
    "    cost_history = []\n",
    "    w_history = []\n",
    "    b_history = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        dw, db = compute_gradients(X, y, w, b)  # you already have this function\n",
    "        w -= alpha * dw\n",
    "        b -= alpha * db\n",
    "        \n",
    "        # Record cost, weights, and bias at intervals\n",
    "        if i % record_interval == 0 or i == iterations - 1:\n",
    "            cost_history.append(compute_cost(X, y, w, b))\n",
    "            w_history.append(w.copy())\n",
    "            b_history.append(b)\n",
    "\n",
    "    return w, b, cost_history, w_history, b_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00850e86",
   "metadata": {},
   "source": "### Training the Model\n\nRunning gradient descent with the following hyperparameters:\n- **Learning rate (α)**: 0.01\n- **Iterations**: 10,000\n- **Recording interval**: 100 iterations"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11f598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=0.01\n",
    "iterations=10000\n",
    "record_interval=100\n",
    "\n",
    "w_final, b_final, cost_history, w_history, b_history = gradient_descent(X_train, y_train, alpha, iterations, record_interval)\n",
    "\n",
    "print(\"Final weights:\", w_final)\n",
    "print(\"Final bias:\", b_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a46e870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct x-axis: length matches cost_history\n",
    "iterations_recorded = [i for i in range(iterations) if i % 100 == 0]\n",
    "if iterations-1 not in iterations_recorded:\n",
    "    iterations_recorded.append(iterations-1)  # include last iteration\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(iterations_recorded, cost_history, marker='o', color='blue')\n",
    "plt.title(\"Cost History during Gradient Descent\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61919554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert w_history to NumPy array for easier indexing\n",
    "w_history = np.array(w_history)  # shape: (num_records, n_features)\n",
    "\n",
    "# x-axis: iteration numbers where history was recorded\n",
    "record_interval = 100\n",
    "iterations_recorded = [i for i in range(0, 10000, record_interval)]\n",
    "if 10000-1 not in iterations_recorded:\n",
    "    iterations_recorded.append(10000-1)  # include last iteration\n",
    "\n",
    "# Plot weights history\n",
    "plt.figure(figsize=(10,6))\n",
    "for i in range(w_history.shape[1]):\n",
    "    plt.plot(iterations_recorded, w_history[:, i], label=f'Weight w{i}')\n",
    "\n",
    "plt.title(\"Weights History During Gradient Descent\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Weight Value\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191588ff",
   "metadata": {},
   "source": "### Model Evaluation\n\nEvaluating the gradient descent model on the validation set using Mean Squared Error (MSE) and R² score metrics."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a19392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on validation set\n",
    "y_val_pred_gd = f_wb(X_val, w, b)\n",
    "\n",
    "# Evaluation\n",
    "mse_gd = np.mean((y_val_pred_gd - y_val)**2)\n",
    "ss_res = np.sum((y_val - y_val_pred_gd)**2)\n",
    "ss_tot = np.sum((y_val - np.mean(y_val))**2)\n",
    "r2_gd = 1 - (ss_res / ss_tot)\n",
    "\n",
    "print(\"Gradient Descent MSE on validation set:\", mse_gd)\n",
    "print(\"Gradient Descent R² score on validation set:\", r2_gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99d206c",
   "metadata": {},
   "source": "## Scikit-learn Linear Regression\n\nTraining a linear regression model using scikit-learn's built-in implementation for comparison with the gradient descent approach."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7951c1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled_skl = scaler.fit_transform(X_train)\n",
    "X_val_scaled_skl = scaler.transform(X_val)\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled_skl, y_train)\n",
    "\n",
    "weights_skl = lr_model.coef_\n",
    "bias_skl = lr_model.intercept_\n",
    "\n",
    "y_val_pred_skl = lr_model.predict(X_val_scaled_skl)\n",
    "\n",
    "mse_skl = mean_squared_error(y_val, y_val_pred_skl)\n",
    "r2_skl_val = r2_score(y_val, y_val_pred_skl)\n",
    "\n",
    "print(\"\\n--- scikit-learn Linear Regression ---\")\n",
    "print(\"Trained weights:\", weights_skl)\n",
    "print(\"Trained bias:\", bias_skl)\n",
    "print(\"MSE on validation set (sklearn):\", mse_skl)\n",
    "print(\"R² score on validation set (sklearn):\", r2_skl_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d37ba17",
   "metadata": {},
   "source": "## Results Comparison\n\nVisualizing the predictions from both models (Gradient Descent and Scikit-learn) against actual values to compare their performance."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9da047",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# Plot actual vs predicted values\n",
    "plt.scatter(y_val, y_val_pred_gd, alpha=0.5, color='blue', label='Gradient Descent')\n",
    "plt.scatter(y_val, y_val_pred_skl, alpha=0.5, color='green', label='scikit-learn')\n",
    "\n",
    "# Plot ideal line\n",
    "plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', label='Ideal')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Actual CO2 Emissions (g/km)\")\n",
    "plt.ylabel(\"Predicted CO2 Emissions (g/km)\")\n",
    "plt.title(\"Actual vs Predicted CO2 Emissions\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}